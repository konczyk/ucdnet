{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 2\n",
    "---\n",
    "<div style=\"margin: -10px 0 20px 0\"><small>Author: Daniel Ko≈Ñczyk (18208152)</small></div>\n",
    "\n",
    "This case study focuses on implementing a community-finding algorithm and comparing it's quality with one from the networkx package. I've decided to implement a `ratio-cut` algorithm with an option to do recursive cuts, from which the best one would be chosen based on modularity scores.\n",
    "\n",
    "The study uses the following software:\n",
    "* Python v3.7\n",
    "* matplotlib package v3.1\n",
    "* numpy package v1.16\n",
    "* networkx package v2.3\n",
    "* pandas package v0.24\n",
    "* scipy package 1.3\n",
    "\n",
    "The notebook has been executed in the following environment:\n",
    "* Linux and macOS (PC and laptop)\n",
    "* 16GB RAM\n",
    "* 4-core CPU\n",
    "\n",
    "This is the most memory-intensive Case Study and can use a lot of RAM. I tried to choose a network that wouldn't cause usage of more than 8GB, but I did not measure it precisely. I wanted to use sparse matrices as in other places, but unfortunately the eigenvectors functions in scipy, mainly `scipy.sparse.linalg.eigsh` turned out unreliable and I didn't have time to investigate further and went back to numpy, which requires a dense input.\n",
    "\n",
    "We start the notebook with importing all required packages and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display_html \n",
    "from networkx.algorithms.community import greedy_modularity_communities, LFR_benchmark_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the quality of communities I use the following modularity function, which is taken straight from lecture notes. This function is also used in the partitioning algorithm to choose the best partition\n",
    "\n",
    "Input:  \n",
    "`A` - numpy array representing a given network  \n",
    "`partitions` - list of partitions of the graph corresponding to A  \n",
    "\n",
    "Returns:  \n",
    "*modularity measure* - a single float numer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modularity(A, partitions):\n",
    "    n, m = A.shape\n",
    "    # 2m - 2 *sum of edges\n",
    "    deg = A.sum()\n",
    "    \n",
    "    # matrix with expected fraction of within-community edges \n",
    "    # for a random graph same degree distribution \n",
    "    B = np.outer(A.sum(axis=1), A.sum(axis=1))/deg\n",
    "    # matrix with partition masks\n",
    "    n, m = A.shape\n",
    "    X = np.zeros((len(partitions),n))\n",
    "    for i,p in enumerate(partitions):\n",
    "        X[i,p]=1\n",
    "     \n",
    "    return np.trace((X.dot(A-B)).dot(X.T))/deg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare communities produced by different algorithms, I've decided to use *Rand Index*, which has a nice vectorized implementation by computing the complement\n",
    "\n",
    "Input:  \n",
    "`part1` - partitioning done by the first algorithm  \n",
    "`part2` - partitioning done by the second algorithm  \n",
    "`n` - numer of nodes in the related graph  \n",
    "\n",
    "Returns:  \n",
    "*Rand Index value* - a single float numer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_index(part1, part2, n):\n",
    "    # create a matrix that lists all the pairs (twice) \n",
    "    # and indicates whether or not the nodes belong to the same partition\n",
    "    def pairs(part):\n",
    "        x = np.zeros(n, dtype=int)\n",
    "        for i,p in enumerate(part):\n",
    "            x[p] = i+1\n",
    "        x = abs(np.array([x]).T - x)   \n",
    "        x[x > 0] = 1\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    group1 = pairs(part1)\n",
    "    group2 = pairs(part2)\n",
    "    \n",
    "    return 1 - abs(group1-group2).sum()/(n*(n-1))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ratio cut* is based on analyzing the eigevector corresponding to the second smallest eigenvalue, called *Fiedler*. This function computes and returns this vector.\n",
    "\n",
    "Input:  \n",
    "`A` - numpy array representing a given network  \n",
    "\n",
    "Returns:  \n",
    "*1d eigenvector* - the Fiedler eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fiedler(A):\n",
    "    n, m = A.shape\n",
    "    # create sparse diagonal degree matrix from A\n",
    "    diags = A.sum(axis=1)\n",
    "    D = sp.sparse.spdiags(diags.flatten(), [0], m, n)\n",
    "    # create Laplacian matrix\n",
    "    L = D-A\n",
    "    # compute eigenvectors of the Laplacian\n",
    "    # since Laplacian is symmetric, use eigh for better performance\n",
    "    # data is returned in sorted order (by eigenvalues), so no need to sort \n",
    "    _, eigen_vec = np.linalg.eigh(L)\n",
    "    \n",
    "    return eigen_vec[:,1].A1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratio cut\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function encapsulates the core of the algorithm - it performs a bipartitioning of the graph based on the sorted Fielder eigenvector, which is then traversed from the highest to the lowest element and the ratio cut is computed. The whole vector is checked and the best ratio and best cut are returned to the caller.\n",
    "\n",
    "Input:  \n",
    "`A` - numpy array representing a given network  \n",
    "\n",
    "Returns:  \n",
    "*a tuple* - best ratio cut value and actual two partions the network was divided to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio_cut(A):\n",
    "    # sort cut vector by\n",
    "    cut_vec = np.argsort(get_fiedler(A))\n",
    "    # create a helper matrix to improve performance of edge counts in partitions\n",
    "    # rows and columns of original matrix are sorted according to cut vector\n",
    "    # this allows standard, continuous numpy indexing instead of advanvced one, which copies data\n",
    "    E = A[:,cut_vec][cut_vec]\n",
    "    # remove self loops\n",
    "    np.fill_diagonal(E, 0)\n",
    "\n",
    "    best_cut, best_ratio = None, None\n",
    "    all_edges, in_edges = 0, 0\n",
    "\n",
    "    # run a loop and find the best cut \n",
    "    for i in range(1,cut_vec.size):\n",
    "        # use E matrix to easily count all/inward/cut edges for a set or nodes\n",
    "        all_edges += E[i-1].sum()\n",
    "        in_edges += 2 * E[i-1,0:i-1].sum()\n",
    "        cut_off_edges = all_edges - in_edges\n",
    "        # compute ratio cut according to formula\n",
    "        ratio_cut = cut_off_edges / min(i, cut_vec.size - i)\n",
    "        \n",
    "        # save the best cut and ratio\n",
    "        if best_ratio is None or best_ratio >= ratio_cut:\n",
    "            best_cut, best_ratio = i, ratio_cut\n",
    "   \n",
    "    return best_ratio, [cut_vec[0:best_cut], cut_vec[best_cut:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since many datasets don't have continuous indices, a simple function is provided to translate between indices (since networkx resets original indices into a continuous space when exporting). This is mostly for reporting, as we may want to see both metrics, but also actual partitions with original node labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(nodes, indices):\n",
    "    return np.nonzero(indices[:,None] == nodes)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function that runs the partitioning. It's fairly simple in a sense, that it strictly makes $2^N$ cuts and exits when a single element partition is created, since it can't be divided any further. There are more limitations, which will be discussed in the summary.\n",
    "\n",
    "The function simply iterates until the depth limit is reached and cuts currently held partitioning one more time. It starts with 1 partition and divides into to 2, then enterd the loop having 2 partitions and further divides them into 4 and so on. While the loop runs, a modularity for the partitioning is recorded, so that only the best partitioning is returned (the one with highest modularity).\n",
    "\n",
    "Input:  \n",
    "`G` - graph to be partitioned  \n",
    "`max_depth` - how many cutting iterations to perform, default is 1 (bipartition)\n",
    "\n",
    "Returns:  \n",
    "*list of partitions* - this is a list of numpy arrays, where each array is one of the partitions of the best cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(G, max_depth=1):\n",
    "    \n",
    "    npa = nx.to_numpy_array(G)\n",
    "    best_cut, best_mod = None, None\n",
    "    partitions = [npa]\n",
    "    nodes = list(G.nodes())\n",
    "    mappings = np.array([nodes])\n",
    "    \n",
    "    while max_depth > 0:\n",
    "        if not (np.array([len(x) for x in partitions]) > 1).all():\n",
    "            return best_cut\n",
    "        \n",
    "        raw_cut = list(itertools.chain(*[ratio_cut(p)[1] for p in partitions]))\n",
    "        cut = [mappings[int(i/2)][k] for i,k in enumerate(raw_cut)]\n",
    "        t_cut = [translate(nodes, c) for c in cut]\n",
    "        mod = modularity(npa, t_cut)\n",
    "        if best_mod is None or best_mod < mod:\n",
    "            best_cut, best_mod = cut, mod\n",
    "        \n",
    "        subgraphs = [G.subgraph(c) for c in cut]\n",
    "        mappings = [np.array(list(s.nodes())) for s in subgraphs]\n",
    "        partitions = [nx.to_numpy_array(s) for s in subgraphs]\n",
    "        max_depth -= 1\n",
    "        \n",
    "    return best_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests and comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm often performs bad on graphs with lots of tiny communities due to its handling of singular partitions. Still, I thought it would be good to include an example from the real world networks to see exactly what is happening. \n",
    "\n",
    "---\n",
    "#### Real World Network\n",
    "I chose `ca-GrQc` from https://snap.stanford.edu/data/ca-GrQc.html which was fairly small to fit in the memory, most of the other undirected graphs on SNAP were just too large fo this version of the algorithm.\n",
    "\n",
    "The code captures modularity of the computed partitions and a rand index between networkx algorithm and my ratio cut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time: 0:01:03\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "G = nx.read_edgelist(\"ca-GrQc.txt\", create_using=nx.Graph(), nodetype=int)\n",
    "p1 = [translate(G.nodes(), np.array(list(p))) for p in greedy_modularity_communities(G)]\n",
    "p2 = [translate(G.nodes(), p) for p in partition(G,16)]\n",
    "\n",
    "A = nx.to_numpy_array(G)\n",
    "nx_modularity = modularity(A,p1)\n",
    "rc_modularity = modularity(A,p2)\n",
    "real_rand_index = rand_index(p1, p2, A.shape[1])\n",
    "print(\"Computation time: {}\".format(datetime.timedelta(seconds=int(time.perf_counter()-s))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can display and discuss the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_be24f6fe_9b64_11e9_8502_10c37b4eae94\" style='display:inline'><caption>Communities comparison (G: nodes 5242)</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >Metric</th>        <th class=\"col_heading level0 col1\" >networkx</th>        <th class=\"col_heading level0 col2\" >ratio-cut</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_be24f6fe_9b64_11e9_8502_10c37b4eae94row0_col0\" class=\"data row0 col0\" >Community Quality (modularity)</td>\n",
       "                        <td id=\"T_be24f6fe_9b64_11e9_8502_10c37b4eae94row0_col1\" class=\"data row0 col1\" >0.8118517</td>\n",
       "                        <td id=\"T_be24f6fe_9b64_11e9_8502_10c37b4eae94row0_col2\" class=\"data row0 col2\" >0.0002070131</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_be24f6fe_9b64_11e9_8502_10c37b4eae94row1_col0\" class=\"data row1 col0\" >Number of partitions</td>\n",
       "                        <td id=\"T_be24f6fe_9b64_11e9_8502_10c37b4eae94row1_col1\" class=\"data row1 col1\" >426</td>\n",
       "                        <td id=\"T_be24f6fe_9b64_11e9_8502_10c37b4eae94row1_col2\" class=\"data row1 col2\" >4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_be24f6fe_9b64_11e9_8502_10c37b4eae94row2_col0\" class=\"data row2 col0\" >Largest partition</td>\n",
       "                        <td id=\"T_be24f6fe_9b64_11e9_8502_10c37b4eae94row2_col1\" class=\"data row2 col1\" >1039</td>\n",
       "                        <td id=\"T_be24f6fe_9b64_11e9_8502_10c37b4eae94row2_col2\" class=\"data row2 col2\" >5238</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_be24f6fe_9b64_11e9_8502_10c37b4eae94row3_col0\" class=\"data row3 col0\" >Smallest partition</td>\n",
       "                        <td id=\"T_be24f6fe_9b64_11e9_8502_10c37b4eae94row3_col1\" class=\"data row3 col1\" >1</td>\n",
       "                        <td id=\"T_be24f6fe_9b64_11e9_8502_10c37b4eae94row3_col2\" class=\"data row3 col2\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_be24f6fe_9b64_11e9_8502_10c37b4eae94row4_col0\" class=\"data row4 col0\" >Communities comparison (rand index, same for both)</td>\n",
       "                        <td id=\"T_be24f6fe_9b64_11e9_8502_10c37b4eae94row4_col1\" class=\"data row4 col1\" >0.06305564</td>\n",
       "                        <td id=\"T_be24f6fe_9b64_11e9_8502_10c37b4eae94row4_col2\" class=\"data row4 col2\" >0.06305564</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame([\n",
    "    [\"Community Quality (modularity)\", nx_modularity, rc_modularity],\n",
    "    [\"Number of partitions\", len(p1), len(p2)],\n",
    "    [\"Largest partition\", max([len(p) for p in p1]),max([len(p) for p in p2])],\n",
    "    [\"Smallest partition\", min([len(p) for p in p1]),min([len(p) for p in p2])],\n",
    "    [\"Communities comparison (rand index, same for both)\", real_rand_index, real_rand_index],\n",
    "    ],\n",
    "    columns=[\"Metric\", \"networkx\", \"ratio-cut\"])\n",
    "\n",
    "df_style = (df.style\n",
    "    .set_table_attributes(\"style='display:inline'\")\n",
    "    .set_caption(\"Communities comparison (G: nodes {})\".format(len(G)))\n",
    "    .hide_index()        \n",
    "    .set_precision(7))\n",
    "display_html(df_style._repr_html_(), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite clear that ratio-cut did not uncover any substantial communities, with alsmot all the nodes sitting in one partition. One of the reason is that the small (2 nodes, exists in networkx partitioning too) partition was uncovered very early and the algorithm had to exit since there was nothing to further cut in this part. The difference is striking and as I mentioned before, some refinements would be needed to make it behave in more intelligent way. Some of these ideas will be discussed in the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small synthetic graph\n",
    "Next test will be run on a small, 500 nodes synthetic graph generated by networkx LFR benchmark, collecting the same data as the previous test, but also checking how well the finding matches the actual communities in the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "G = LFR_benchmark_graph(n=500, tau1=3, tau2=1.5, mu=0.1, average_degree=5, min_community=60, seed=10)\n",
    "p1 = [translate(G.nodes(), np.array(list(p))) for p in greedy_modularity_communities(G)]\n",
    "p2 = [translate(G.nodes(), p) for p in partition(G,16)]\n",
    "p3 = np.unique([list(G.nodes[v]['community']) for v in G])\n",
    "\n",
    "A = nx.to_numpy_array(G)\n",
    "nx_modularity = modularity(A,p1)\n",
    "rc_modularity = modularity(A,p2)\n",
    "lfr_modularity = modularity(A,p3)\n",
    "rand_index_p1p2 = rand_index(p1, p2, A.shape[1])\n",
    "rand_index_p1p3 = rand_index(p1, p3, A.shape[1])\n",
    "rand_index_p2p3 = rand_index(p2, p3, A.shape[1])\n",
    "print(\"Computation time: {}\".format(datetime.timedelta(seconds=int(time.perf_counter()-s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94\" style='display:inline;width:100%'><caption>Communities comparison (G: nodes 500, communities 4,                  largest partition 149, smallest partition 103)</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >Metric</th>        <th class=\"col_heading level0 col1\" >networkx</th>        <th class=\"col_heading level0 col2\" >ratio-cut</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94row0_col0\" class=\"data row0 col0\" >Community Quality (modularity)</td>\n",
       "                        <td id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94row0_col1\" class=\"data row0 col1\" >0.6655362</td>\n",
       "                        <td id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94row0_col2\" class=\"data row0 col2\" >0.659621</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94row1_col0\" class=\"data row1 col0\" >Number of partitions</td>\n",
       "                        <td id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94row1_col1\" class=\"data row1 col1\" >4</td>\n",
       "                        <td id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94row1_col2\" class=\"data row1 col2\" >4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94row2_col0\" class=\"data row2 col0\" >Largest partition</td>\n",
       "                        <td id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94row2_col1\" class=\"data row2 col1\" >147</td>\n",
       "                        <td id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94row2_col2\" class=\"data row2 col2\" >148</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94row3_col0\" class=\"data row3 col0\" >Smallest partition</td>\n",
       "                        <td id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94row3_col1\" class=\"data row3 col1\" >102</td>\n",
       "                        <td id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94row3_col2\" class=\"data row3 col2\" >101</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94row4_col0\" class=\"data row4 col0\" >Found communities comparison (rand index, same for both)</td>\n",
       "                        <td id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94row4_col1\" class=\"data row4 col1\" >0.9779238</td>\n",
       "                        <td id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94row4_col2\" class=\"data row4 col2\" >0.9779238</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94row5_col0\" class=\"data row5 col0\" >Found communities comparison with actual data (rand index, different for both)</td>\n",
       "                        <td id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94row5_col1\" class=\"data row5 col1\" >0.9934669</td>\n",
       "                        <td id=\"T_be6f963c_9b64_11e9_8502_10c37b4eae94row5_col2\" class=\"data row5 col2\" >0.971503</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame([\n",
    "    [\"Community Quality (modularity)\", nx_modularity, rc_modularity],\n",
    "    [\"Number of partitions\", len(p1), len(p2)],\n",
    "    [\"Largest partition\", max([len(p) for p in p1]),max([len(p) for p in p2])],\n",
    "    [\"Smallest partition\", min([len(p) for p in p1]),min([len(p) for p in p2])],\n",
    "    [\"Found communities comparison (rand index, same for both)\", rand_index_p1p2, rand_index_p1p2],\n",
    "    [\"Found communities comparison with actual data (rand index, different for both)\", rand_index_p1p3, rand_index_p2p3],\n",
    "    ],\n",
    "    columns=[\"Metric\", \"networkx\", \"ratio-cut\"])\n",
    "\n",
    "df_style = (df.style\n",
    "    .set_table_attributes(\"style='display:inline;width:100%'\")\n",
    "    .set_caption(\"Communities comparison (G: nodes {}, communities {}, \\\n",
    "                 largest partition {}, smallest partition {})\".format(len(G), len(p3), \n",
    "                                                                      max([len(p) for p in p3]), \n",
    "                                                                      min([len(p) for p in p3])))\n",
    "    .hide_index()        \n",
    "    .set_precision(7))\n",
    "display_html(df_style._repr_html_(), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this synthetic graph, the outlook is much different. Both algorithms partitioned the graph almost perfectly as seen in rand index both between the algorithms and the actual communities in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Large synthetic graph\n",
    "Next test will be run on a larger, 5000 nodes synthetic graph generated by networkx LFR benchmark, collecting the same data as the previous test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time: 0:02:18\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "G = LFR_benchmark_graph(n=5000, tau1=2.8, tau2=1.5, mu=0.15, average_degree=25, min_community=80, seed=10)\n",
    "p1 = [translate(G.nodes(), np.array(list(p))) for p in greedy_modularity_communities(G)]\n",
    "p2 = [translate(G.nodes(), p) for p in partition(G,16)]\n",
    "p3 = np.unique([list(G.nodes[v]['community']) for v in G])\n",
    "\n",
    "A = nx.to_numpy_array(G)\n",
    "nx_modularity = modularity(A,p1)\n",
    "rc_modularity = modularity(A,p2)\n",
    "lfr_modularity = modularity(A,p3)\n",
    "rand_index_p1p2 = rand_index(p1, p2, A.shape[1])\n",
    "rand_index_p1p3 = rand_index(p1, p3, A.shape[1])\n",
    "rand_index_p2p3 = rand_index(p2, p3, A.shape[1])\n",
    "print(\"Computation time: {}\".format(datetime.timedelta(seconds=int(time.perf_counter()-s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_11296948_9b65_11e9_8502_10c37b4eae94\" style='display:inline;width:100%'><caption>Communities comparison (G: nodes 5000, communities 14,                  largest partition 902, smallest partition 85)</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >Metric</th>        <th class=\"col_heading level0 col1\" >networkx</th>        <th class=\"col_heading level0 col2\" >ratio-cut</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_11296948_9b65_11e9_8502_10c37b4eae94row0_col0\" class=\"data row0 col0\" >Community Quality (modularity)</td>\n",
       "                        <td id=\"T_11296948_9b65_11e9_8502_10c37b4eae94row0_col1\" class=\"data row0 col1\" >0.6215929</td>\n",
       "                        <td id=\"T_11296948_9b65_11e9_8502_10c37b4eae94row0_col2\" class=\"data row0 col2\" >0.2060774</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_11296948_9b65_11e9_8502_10c37b4eae94row1_col0\" class=\"data row1 col0\" >Number of partitions</td>\n",
       "                        <td id=\"T_11296948_9b65_11e9_8502_10c37b4eae94row1_col1\" class=\"data row1 col1\" >8</td>\n",
       "                        <td id=\"T_11296948_9b65_11e9_8502_10c37b4eae94row1_col2\" class=\"data row1 col2\" >16</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_11296948_9b65_11e9_8502_10c37b4eae94row2_col0\" class=\"data row2 col0\" >Largest partition</td>\n",
       "                        <td id=\"T_11296948_9b65_11e9_8502_10c37b4eae94row2_col1\" class=\"data row2 col1\" >909</td>\n",
       "                        <td id=\"T_11296948_9b65_11e9_8502_10c37b4eae94row2_col2\" class=\"data row2 col2\" >4079</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_11296948_9b65_11e9_8502_10c37b4eae94row3_col0\" class=\"data row3 col0\" >Smallest partition</td>\n",
       "                        <td id=\"T_11296948_9b65_11e9_8502_10c37b4eae94row3_col1\" class=\"data row3 col1\" >96</td>\n",
       "                        <td id=\"T_11296948_9b65_11e9_8502_10c37b4eae94row3_col2\" class=\"data row3 col2\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_11296948_9b65_11e9_8502_10c37b4eae94row4_col0\" class=\"data row4 col0\" >Found communities comparison (rand index, same for both)</td>\n",
       "                        <td id=\"T_11296948_9b65_11e9_8502_10c37b4eae94row4_col1\" class=\"data row4 col1\" >0.4323737</td>\n",
       "                        <td id=\"T_11296948_9b65_11e9_8502_10c37b4eae94row4_col2\" class=\"data row4 col2\" >0.4323737</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_11296948_9b65_11e9_8502_10c37b4eae94row5_col0\" class=\"data row5 col0\" >Found communities comparison with actual data (rand index, different for both)</td>\n",
       "                        <td id=\"T_11296948_9b65_11e9_8502_10c37b4eae94row5_col1\" class=\"data row5 col1\" >0.9617047</td>\n",
       "                        <td id=\"T_11296948_9b65_11e9_8502_10c37b4eae94row5_col2\" class=\"data row5 col2\" >0.4477797</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame([\n",
    "    [\"Community Quality (modularity)\", nx_modularity, rc_modularity],\n",
    "    [\"Number of partitions\", len(p1), len(p2)],\n",
    "    [\"Largest partition\", max([len(p) for p in p1]),max([len(p) for p in p2])],\n",
    "    [\"Smallest partition\", min([len(p) for p in p1]),min([len(p) for p in p2])],\n",
    "    [\"Found communities comparison (rand index, same for both)\", rand_index_p1p2, rand_index_p1p2],\n",
    "    [\"Found communities comparison with actual data (rand index, different for both)\", rand_index_p1p3, rand_index_p2p3],\n",
    "    ],\n",
    "    columns=[\"Metric\", \"networkx\", \"ratio-cut\"])\n",
    "\n",
    "df_style = (df.style\n",
    "    .set_table_attributes(\"style='display:inline;width:100%'\")\n",
    "    .set_caption(\"Communities comparison (G: nodes {}, communities {}, \\\n",
    "                 largest partition {}, smallest partition {})\".format(len(G), len(p3), \n",
    "                                                                      max([len(p) for p in p3]), \n",
    "                                                                      min([len(p) for p in p3])))\n",
    "    .hide_index()        \n",
    "    .set_precision(7))\n",
    "display_html(df_style._repr_html_(), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time ratio-cut found more communities that were actually there and as before had to stop with 1-node partition. It is quite often the case, that ratio cut partitions the graph in quite uneven halves, resulting in a very large community at the end of the process. This is a potential for another change, where such disproportion would be detected and extra cut scenarios deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio cut is often a good choice for an initial bipartinioning of the graph, but seems to sometimes fall short in a recursive scenario, requiring much more tuning than doing only recursive cuts. Despite the limitations, it is still able to find communities in some of the graphs, which seems quite surprising given it's rather simple structure. With a few enhancements, the quality of the algorithm could be vastly improved. I've already noted a few things I will most likely re-visit in the future:\n",
    "* Don't stop if one of the partitions is small (1 node), but cut the other ones. This will result in a partitioning not bounded by $2^N$ and could uncover extra communities. The algorithm very often quits early, losing all that potential\n",
    "* Perform ratio cuts in parallel - this could considerably speed up the algorithm for larger graphs\n",
    "* Re-visit using sparse matrices - the current cost of computations is extremely high, but I couldn't find a reliable to compute eigenvalues on the sparse matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
