{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 1\n",
    "---\n",
    "<div style=\"margin: -10px 0 20px 0\"><small>Author: Daniel Kończyk (18208152)</small></div>\n",
    "\n",
    "This case study focuses on network analysis and modeling by using graphs taken from the `SNAP` repository.\n",
    "\n",
    "The study uses the following software:\n",
    "* Python v3.7\n",
    "* matplotlib package v3.1\n",
    "* numpy package v1.16\n",
    "* networkx package v2.3\n",
    "\n",
    "The notebook has been executed on the following environment:\n",
    "* Linux and macOS (PC and laptop)\n",
    "* 16GB RAM\n",
    "* 4-core CPU\n",
    "\n",
    "I've selected fairly large graphs for analysis to make this case study a bit more challenging, but unfortunately this also results in **fairly high computation time** in some parts of the study. These computations are generally CPU-intensive and memory usage should be fairly low (less than 8GB). They have been wrapped in timing code to clearly show the expected runtime and actual values have been put in comments to skip the compuation altogether, if needed. \n",
    "\n",
    "We start the notebook with importing all required packages and modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs were downloaded from the `SNAP` repository and stored locally for convenience.  \n",
    "* For directed graph **Stanford web graph** was chosen and downloaded from https://snap.stanford.edu/data/web-Stanford.html\n",
    "* For undirected graph **Autonomous systems by Skitter** was chosen and downloaded from https://snap.stanford.edu/data/as-Skitter.html\n",
    "\n",
    "The references to the downloaded and unpacked files are then stored in the following variables for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directed_graph_file = \"web-Stanford.txt\"\n",
    "undirected_graph_file = \"email-Enron.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  “Broder et al” butterfly picture of the directed network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with computing Strongly Connected Components (SCC) in the directed graph of choice. While there are ready-made functions in networkx, I've decided to implement one of the known algorithms (*Kosaraju's algorithm*) and use my own simple data structure to represent the graph.\n",
    "\n",
    "For the purpose of SCC computation, the graph is represented as an adjacency list, where each key represents a node and value represents its neighbours as a set of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = defaultdict(set)\n",
    "# Load data into the graph data structure\n",
    "with open(directed_graph_file, \"r\") as ins:\n",
    "    for line in ins:\n",
    "        if not line.startswith(\"#\"):\n",
    "            u, v = [int(i) for i in line.split()]\n",
    "            G[u].add(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, I chose to implement *Kosaraju's algorithm* to compute SCC in the directed graph. The complexity of this algorithm is $O(E+V)$. The main idea here is that the SCCs in graph G are the same as in the transpose of G (direction of every edge reversed).  \n",
    "The implementation is much cleaner with recursive DFS, but this often fails in large graphs due to stack overflows, so I decided to implement an iterative DFS version from the start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SCC\n",
    "def scc(G):\n",
    "    \"\"\"\n",
    "    Run Depth First Search to find out the finish times \n",
    "    to find each vertex in graph G.\n",
    "    Pass the visited set in case we have a disconnected graph\n",
    "    \"\"\"\n",
    "    visited, finish_times = set(), []\n",
    "    for u in list(G.keys()):\n",
    "        if u not in visited:\n",
    "            finish_times.extend([v for v in dfs(G, u, visited)])\n",
    "    \n",
    "    # Reverse graph G\n",
    "    GR = defaultdict(set)\n",
    "    for u, nodes in G.items():\n",
    "        for v in nodes:\n",
    "            GR[v].add(u)\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute strongly connected components by running Depth First Search\n",
    "    on a reversed graph G, by exploring in reversed finish_times order.\n",
    "    Pass the visited set to no explore the same components multiple times\n",
    "    \"\"\"\n",
    "    visited, scc = set(), defaultdict(set)\n",
    "    for u in reversed(finish_times):\n",
    "        if u not in visited:\n",
    "            scc[u].update([node for node in dfs(GR, u, visited)])\n",
    "    return scc\n",
    "\n",
    "# Run iterative DFS to prevent stack overflow in large graphs\n",
    "def dfs(G, source, visited):\n",
    "    stack, popped = [source], set()\n",
    "    while stack:\n",
    "        node = stack[-1]\n",
    "        to_visit = G[node] - visited\n",
    "        if node not in visited:\n",
    "            visited.add(node)\n",
    "            stack.extend(to_visit)\n",
    "        else:\n",
    "            stack.pop()\n",
    "            if node not in popped:\n",
    "                popped.add(node)\n",
    "                yield node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to compute the SCC and extract the giant (largest SCC) component. Additionaly we extract the nodes that are not in the giant SCC to make further computations easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time: 0:00:17\n"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "components = sorted(scc(G).values(), key=len)\n",
    "# largest SCC\n",
    "giant = components[-1]\n",
    "# nodes not in largest SCC\n",
    "not_giant = reduce(set.union, components[:-1])\n",
    "print(\"Computation time: {}\".format(datetime.timedelta(seconds=int(time.perf_counter()-s))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the components computed, we are ready to find the IN and OUT sections of the graph.  \n",
    "* The size of an IN section can be found by iterating over the nodes outside the giant SCC and checking for intersection of their neighbours with the giant SCC\n",
    "* The size of an OUT section can be found by iterating over the nodes in the giant SCC and checking for intersection of their neighbours with nodes outside the giant SCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_nodes = 0\n",
    "for u in not_giant:\n",
    "    if G[u].intersection(giant):\n",
    "        in_nodes += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_nodes = 0\n",
    "for u in giant:\n",
    "    if G[u].intersection(not_giant):\n",
    "        out_nodes += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print out the results for this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes in the giant SCC: 150532\n",
      "Nodes in the IN section of the graph: 26239\n",
      "Nodes in the OUT section of the grap: 2604\n"
     ]
    }
   ],
   "source": [
    "print(\"Nodes in the giant SCC: {}\".format(len(giant)))\n",
    "print(\"Nodes in the IN section of the graph: {}\".format(in_nodes))\n",
    "print(\"Nodes in the OUT section of the grap: {}\".format(out_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The parameter β of the power-law degree distribution model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part of the study focuses on fitting a line to a log-log plot of the degree distribution of both graphs and computing its slope to determine the parameter $\\beta$ of the power-law degree distribution model.\n",
    "\n",
    "We start by loading both graphs from files, this time using `networkx` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'directed_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b593d14217a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_edgelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirected_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_using\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodetype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_edgelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mundirected_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_using\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodetype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading time: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'directed_graph' is not defined"
     ]
    }
   ],
   "source": [
    "s = time.perf_counter()\n",
    "D = nx.read_edgelist(directed_graph_file, create_using=nx.DiGraph(), nodetype=int)\n",
    "G = nx.read_edgelist(undirected_graph_file, create_using=nx.Graph(), nodetype=int)\n",
    "print(\"Loading time: {}\".format(datetime.timedelta(seconds=int(time.perf_counter()-s))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter $\\beta$ can be easily determined from the CCDF, where $P(D_v>d)=\\big(\\frac{d}{d_{min}}\\big)^{-\\beta}$ and $\\beta = \\alpha-1$.  \n",
    "A few functions to compute the necessary data are created:\n",
    "* `gen_ccdf` computes CCDF from the degree view of a graph, returning a tuple with lists of $x$ and $y$ values, where $x$ are degrees of the graph and $y$ are the associated probabilities of the CCDF. This function adds 1 to each degree if the smallest degree is 0 to avoid problems with log function\n",
    "* `plot_ccdf` plots the CCDF (x,y values) along with the fitted line (fit_coeffs)\n",
    "* `fit_cdf` fits a line to a log-log of the CCDF by using *polyfit* function from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ccdf(degrees):\n",
    "    hist = {}\n",
    "    c = 0\n",
    "    if min([d for _, d in degrees]) == 0:\n",
    "        c = 1    \n",
    "    for d in [d+c for _, d in degrees]:\n",
    "        hist[d] = hist.get(d,0) + 1\n",
    "\n",
    "    cumsum = 0\n",
    "    ccdf = {}\n",
    "    for d in sorted(hist.keys(), reverse=True):\n",
    "        cumsum += hist[d]\n",
    "        ccdf[d] = cumsum/len(degrees)\n",
    "    x,y = zip(*(ccdf.items()))\n",
    "    return np.array(x), np.array(y)\n",
    "        \n",
    "def plot_ccdf(x, y, fit_coeffs, title):\n",
    "    plt.loglog(x,y)\n",
    "    x1 = sorted(x)\n",
    "    poly = np.poly1d(fit_coeffs)\n",
    "    yfit = lambda x: np.exp(poly(np.log(x)))\n",
    "    plt.loglog(x1,yfit(x1),\"--\",color=\"r\")\n",
    "    plt.ylim(1e-6)\n",
    "    plt.xlabel(\"in-degree ($d$)\")\n",
    "    plt.ylabel(\"$P(D_v > d)$\")\n",
    "    plt.title(title)\n",
    "\n",
    "def fit_ccdf(x, y):\n",
    "    return np.polyfit(np.log(x), np.log(y), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to plot the CCDF of the directed graph and compute the slop and paramter $\\beta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = gen_ccdf(D.in_degree())\n",
    "coeffs = fit_ccdf(x,y)\n",
    "plot_ccdf(x, y, coeffs, \"In-degree distribution of the directed graph\")\n",
    "plt.show()\n",
    "D_beta = -coeffs[0]\n",
    "D_alpha = D_beta+1\n",
    "print(\"Parameter β: {}\".format(D_beta))\n",
    "print(\"Parameter α: {}\".format(D_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the same plot of the CCDF and the fit line but for the undirected graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = gen_ccdf(G.degree())\n",
    "coeffs = fit_ccdf(x,y)\n",
    "plot_ccdf(x, y, coeffs, \"Degree distribution of the undirected graph\")\n",
    "plt.show()\n",
    "G_beta = -coeffs[0]\n",
    "G_alpha = G_beta+1\n",
    "print(\"Parameter β: {}\".format(G_beta))\n",
    "print(\"Parameter α: {}\".format(G_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most of the metrix `networkx` is used, but due to quite large graphs being selected, computing average path lengths was impossible on the full set of nodes (test run was executing for a few hours before I stopped it). Thus an auxillary function was provided to compute this metric only on a $N$ sample of nodes. The seed is used to get consistent results over multiple runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_shortest_path(G, N):\n",
    "    visited = set()\n",
    "    nodes = list(G.nodes())\n",
    "    np.random.seed(12345)\n",
    "    np.random.shuffle(nodes)\n",
    "    avg = []\n",
    "    for n in range(N):\n",
    "        node = nodes[n]\n",
    "        visited.add(node)\n",
    "        lengths = []\n",
    "        for s, l in nx.single_source_shortest_path_length(G,node).items():\n",
    "            if s != node and G.is_directed() is True or s not in visited:\n",
    "                lengths.append(l)\n",
    "        if len(lengths) > 0:\n",
    "            avg.append(np.mean(lengths))\n",
    "    return np.mean(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was still curious about the actual values for the average path, so I decided to use a simple `R` script using `igraph` library:\n",
    "```R\n",
    "library(igraph)\n",
    "d <- read.table(\"web-Stanford.txt\",header=F,sep=\"\\t\",comment.char=\"#\")\n",
    "g = graph_from_edgelist(as.matrix(d[,1:2, drop = FALSE], directed = TRUE))\n",
    "mean_distance(g, directed=TRUE)\n",
    "```\n",
    "Turns out R was able to compute it in about 1.5h and the value was XXX.\n",
    "\n",
    "Nevertheless, I added pre-computed values for some of the variables in order to save time on the re-runs, since each computation takes about 15 minutes. We can just comment out actual code invocations if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.perf_counter()\n",
    "D_assortativity = nx.degree_pearson_correlation_coefficient(D)\n",
    "# pre-computed value\n",
    "# D_clustering = 0.5976304608024073\n",
    "D_clustering = nx.average_clustering(D.to_undirected())\n",
    "# pre-computed value\n",
    "# D_shortest_path = 16.109514211378873\n",
    "D_shortest_path = avg_shortest_path(D, 2000)\n",
    "print(\"Computation time: {}\".format(datetime.timedelta(seconds=int(time.perf_counter()-s))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can report the metrics for the **directed** graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Degree Assortativity Coefficient:\\t{}\".format(D_assortativity))\n",
    "print(\"Clustering Coefficient:\\t\\t\\t{}\".format(D_clustering))\n",
    "print(\"Average path length (sampled):\\t\\t{}\".format(D_shortest_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Similarly to the directed graph, I was crious about the actual values for the average path in the undirected graph too, which was not feasable to compute in Python, so I decided to use the same `R` script:\n",
    "```R\n",
    "library(igraph)\n",
    "d <- read.table(\"as-skitter.txt\",header=F,sep=\"\\t\",comment.char=\"#\")\n",
    "g <- graph_from_data_frame(d, directed = TRUE)\n",
    "mean_distance(g, directed=TRUE)\n",
    "```\n",
    "R was able to compute it in about XXX and the value was XXX.\n",
    "\n",
    "Again, I added pre-computed values for some of the variables in order to save time on the re-runs and can just comment the actual code invocations if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.perf_counter()\n",
    "G_assortativity = nx.degree_pearson_correlation_coefficient(G)\n",
    "G_clustering = nx.average_clustering(G)\n",
    "G_shortest_path = avg_shortest_path(G, 2000)\n",
    "print(\"Computation time: {}\".format(datetime.timedelta(seconds=int(time.perf_counter()-s))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can report the metrics for the **undirected** graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Degree Assortativity Coefficient:\\t{}\".format(G_assortativity))\n",
    "print(\"Clustering Coefficient:\\t\\t\\t{}\".format(G_clustering))\n",
    "print(\"Average path length (sampled):\\t\\t{}\".format(G_shortest_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
